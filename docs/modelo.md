# Actividad DemocrÃ¡tica con K Vecinos MÃ¡s Cercanos

## ImplementaciÃ³n Manual de k-NN para PredicciÃ³n de Voto Electoral

<div align="center" style="margin:16px 0;">
  <div style="display:inline-flex;align-items:center;gap:12px;padding:12px 16px;border-radius:8px;
              background:#f6f8fa;border:1px solid #e1e4e8;box-shadow:0 1px 2px rgba(0,0,0,0.03);">
    <img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png"
         alt="GitHub" width="40" height="40" style="flex:0 0 40px;">
    <div style="text-align:left;">
      <div style="font-weight:700;font-size:1.05rem;color:#24292f;">
        <a href="https://github.com/ThissFredy/KNN-VOTANTES-backend.git" target="_blank" rel="noopener" style="color:inherit;text-decoration:none;">
          KNN-VOTANTES-backend
        </a>
      </div>
      <div style="color:#57606a;font-size:0.9rem;">
        Repositorio Â· API kâ€‘NN de intenciÃ³n de voto
      </div>
    </div>
  </div>
</div>
<div align="center" style="margin:16px 0;">
  <div style="display:inline-flex;align-items:center;gap:12px;padding:12px 16px;border-radius:8px;
              background:#f6f8fa;border:1px solid #e1e4e8;box-shadow:0 1px 2px rgba(0,0,0,0.03);">
    <img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png"
         alt="GitHub" width="40" height="40" style="flex:0 0 40px;">
    <div style="text-align:left;">
      <div style="font-weight:700;font-size:1.05rem;color:#24292f;">
        <a href="https://github.com/ThissFredy/knn-votantes-frontend.git" target="_blank" rel="noopener" style="color:inherit;text-decoration:none;">
          knn-votantes-frontend
        </a>
      </div>
      <div style="color:#57606a;font-size:0.9rem;">
        Repositorio Â· API kâ€‘NN de intenciÃ³n de voto
      </div>
    </div>
  </div>
</div>

<div align="center" style="margin:16px 0;">
  <div style="display:inline-flex;align-items:center;gap:12px;padding:12px 16px;border-radius:8px;
              background:#f6f8fa;border:1px solid #e1e4e8;box-shadow:0 1px 2px rgba(0,0,0,0.03);">
    <img src="https://colab.research.google.com/img/colab_favicon_256px.png"
         alt="GitHub" width="40" height="40" style="flex:0 0 40px;">
    <div style="text-align:left;">
      <div style="font-weight:700;font-size:1.05rem;color:#24292f;">
        <a href="https://colab.research.google.com/drive/1HeS-y8HrfYeNbaTmXbrkJq6UfNb650ry?usp=sharing" target="_blank" rel="noopener" style="color:inherit;text-decoration:none;">
          MODELO-KNN-VOTANTES
        </a>
      </div>
      <div style="color:#57606a;font-size:0.9rem;">
        Notebook Â· ImplementaciÃ³n Manual de k-NN
      </div>
    </div>
  </div>
</div>



---

**Curso:** Machine Learning  
**Programa:** IngenierÃ­a en Sistemas y ComputaciÃ³n  
**Integrantes:** Fredy Alejandro Zarate, Juan David Rodriguez

## 1. Resumen Ejecutivo

El algoritmo **k-Nearest Neighbors (k-NN)** es un mÃ©todo de clasificaciÃ³n supervisada que clasifica nuevas instancias basÃ¡ndose en la votaciÃ³n mayoritaria de sus k vecinos mÃ¡s cercanos en el espacio de caracterÃ­sticas. Este proyecto implementa k-NN **completamente desde cero** para predecir el voto electoral de ciudadanos.

### 1.2 Resultados Obtenidos

| MÃ©trica              | Valor                   |
| -------------------- | ----------------------- |
| **Accuracy**         | **93.18%**              |
| **F1-Score (macro)** | **0.9129**              |
| **k Ã³ptimo**         | **19**                  |
| **Dataset**          | 3,000 votantes          |
| **Clases**           | 10 candidatos           |
| **Features**         | 46 variables procesadas |

### 1.3 Logros Destacados

âœ… **ImplementaciÃ³n manual completa** sin uso de `sklearn.KNeighborsClassifier`  
âœ… **OptimizaciÃ³n rigurosa de k** mediante experimentaciÃ³n (k=1 hasta k=19)  
âœ… **Alta precisiÃ³n**: 93.18% de accuracy en conjunto de prueba  
âœ… **JustificaciÃ³n matemÃ¡tica** de cada decisiÃ³n tÃ©cnica  
âœ… **Arquitectura escalable** con base de datos PostgreSQL en Render

---

## 2. Fundamento TeÃ³rico de k-NN

### 2.1 DefiniciÃ³n Formal

Dado un conjunto de entrenamiento $\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^{n}$ donde:

-   $\mathbf{x}_i \in \mathbb{R}^d$ son vectores de caracterÃ­sticas (d=46 en nuestro caso)
-   $y_i \in \{0, 1, ..., 9\}$ son las etiquetas de clase (10 candidatos)

Para clasificar un nuevo punto $\mathbf{x}_{nuevo}$, el algoritmo k-NN:

**1. Calcula distancias** desde $\mathbf{x}_{nuevo}$ a todos los puntos en $\mathcal{D}$:

$$d_i = ||\mathbf{x}_{nuevo} - \mathbf{x}_i||_2 = \sqrt{\sum_{j=1}^{d} (x_{nuevo,j} - x_{i,j})^2}$$

**2. Selecciona los k puntos mÃ¡s cercanos**: $\mathcal{N}_k(\mathbf{x}_{nuevo})$

**3. Predice mediante votaciÃ³n mayoritaria:**

Si k/2 + 1 votos para la clase c

**4. Calcula confianza de predicciÃ³n:**

$$\text{confianza} = \frac{\max(\text{votos})}{k}$$

## 3. Arquitectura del Sistema

### 3.1 Diagrama de Flujo General

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FASE 1: CARGA Y PREPARACIÃ“N                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  PostgreSQL (Render)                                             â”‚
â”‚  3000 votantes Ã— 29 vars                                         â”‚
â”‚          â†“                                                       â”‚
â”‚  get_csv_from_postgresql()                                       â”‚
â”‚          â†“                                                       â”‚
â”‚  Separar X (features) e y (intended_vote)                        â”‚
â”‚          â†“                                                       â”‚
â”‚  Descargar preprocesador desde GitHub                            â”‚
â”‚  (final_preprocessor.joblib)                                     â”‚
â”‚          â†“                                                       â”‚
â”‚  Aplicar transformaciones:                                       â”‚
â”‚  â€¢ StandardScaler â†’ vars continuas (6)                           â”‚
â”‚  â€¢ StandardScaler â†’ vars ordinales (20)                          â”‚
â”‚  â€¢ OneHotEncoder â†’ vars nominales (20) â†’ 46 features finales    â”‚
â”‚          â†“                                                       â”‚
â”‚  Train/Test Split (80/20, stratified)                            â”‚
â”‚  â€¢ X_train: 2400 Ã— 46                                            â”‚
â”‚  â€¢ X_test: 600 Ã— 46                                              â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  FASE 2: PREDICCIÃ“N k-NN MANUAL                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Nuevo Votante (dict con 29 variables originales)               â”‚
â”‚          â†“                                                       â”‚
â”‚  preprocessor.transform()                                        â”‚
â”‚          â†“                                                       â”‚
â”‚  Vector numpy (1 Ã— 46)                                           â”‚
â”‚          â†“                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  ALGORITMO k-NN (k=19)                           â”‚           â”‚
â”‚  â”‚                                                   â”‚           â”‚
â”‚  â”‚  1. Calcular 2400 distancias euclidianas         â”‚           â”‚
â”‚  â”‚     d_i = ||x_nuevo - x_i||â‚‚                     â”‚           â”‚
â”‚  â”‚                                                   â”‚           â”‚
â”‚  â”‚  2. Ordenar distancias (menor â†’ mayor)           â”‚           â”‚
â”‚  â”‚     [(dâ‚, yâ‚), (dâ‚‚, yâ‚‚), ..., (dâ‚‚â‚„â‚€â‚€, yâ‚‚â‚„â‚€â‚€)]   â”‚           â”‚
â”‚  â”‚                                                   â”‚           â”‚
â”‚  â”‚  3. Seleccionar k=19 vecinos mÃ¡s cercanos        â”‚           â”‚
â”‚  â”‚     k_vecinos = [yâ‚, yâ‚‚, ..., yâ‚â‚‰]              â”‚           â”‚
â”‚  â”‚                                                   â”‚           â”‚
â”‚  â”‚  4. VotaciÃ³n mayoritaria (Counter)               â”‚           â”‚
â”‚  â”‚     {0: 7, 1: 8, 2: 4} â†’ pred=1 (8 votos)       â”‚           â”‚
â”‚  â”‚                                                   â”‚           â”‚
â”‚  â”‚  5. Calcular confianza                           â”‚           â”‚
â”‚  â”‚     confianza = 8/19 = 42.1%                     â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚          â†“                                                       â”‚
â”‚  PredicciÃ³n: cÃ³digo numÃ©rico (0-9)                              â”‚
â”‚          â†“                                                       â”‚
â”‚  target_map[predicciÃ³n] â†’ "CAND_Gaia"                           â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FASE 3: EVALUACIÃ“N Y MÃ‰TRICAS                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Predecir en X_test (600 votantes)                              â”‚
â”‚          â†“                                                       â”‚
â”‚  Comparar predicciones vs y_test                                â”‚
â”‚          â†“                                                       â”‚
â”‚  ğŸ“Š MÃ©tricas finales:                                            â”‚
â”‚  â€¢ Accuracy: 93.18%                                              â”‚
â”‚  â€¢ F1-Score (macro): 0.9129                    â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Stack TecnolÃ³gico

| Componente           | TecnologÃ­a                | PropÃ³sito                       |
| -------------------- | ------------------------- | ------------------------------- |
| **Base de Datos**    | PostgreSQL (Render)       | Almacenamiento de 3000 votantes |
| **ORM**              | SQLAlchemy                | ConexiÃ³n y consultas SQL        |
| **Preprocesamiento** | scikit-learn              | StandardScaler + OneHotEncoder  |
| **CÃ³mputo numÃ©rico** | NumPy                     | Operaciones vectorizadas        |
| **EvaluaciÃ³n**       | scikit-learn.metrics      | Accuracy, F1-Score              |
| **k-NN**             | **ImplementaciÃ³n manual** | Sin sklearn.neighbors           |

---

## 4. Dataset y Preprocesamiento

### 4.1 DescripciÃ³n del Dataset

**Fuente:** Base de datos PostgreSQL hospedada en Render  
**URL:** `dpg-d4a9hfbipnbc739gsrpg-a.oregon-postgres.render.com/dbknn`  
**Tabla:** `datos`  
**TamaÃ±o:** **3,000 votantes**  
**Features originales:** 29 variables  
**Features despuÃ©s de preprocesamiento:** 46 variables  
**Variable objetivo:** `intended_vote` (voto intencional)

### 4.2 DistribuciÃ³n de Clases

El dataset presenta un **desbalance moderado** entre candidatos:

| Candidato         | Votos | Porcentaje | CÃ³digo |
| ----------------- | ----- | ---------- | ------ |
| **CAND_Gaia**     | 676   | 22.5%      | 6      |
| **CAND_Azon**     | 541   | 18.0%      | 0      |
| **CAND_Demetra**  | 474   | 15.8%      | 3      |
| **CAND_Civico**   | 311   | 10.4%      | 2      |
| **CAND_Electra**  | 263   | 8.8%       | 4      |
| **CAND_Jade**     | 187   | 6.2%       | 9      |
| **CAND_Icaro**    | 158   | 5.3%       | 8      |
| **CAND_Frontera** | 133   | 4.4%       | 5      |
| **CAND_Boreal**   | 132   | 4.4%       | 1      |
| **CAND_Halley**   | 125   | 4.2%       | 7      |

**ImplicaciÃ³n:** El uso de `stratify=y` en `train_test_split` asegura que Train y Test mantengan esta distribuciÃ³n.

### 4.3 JustificaciÃ³n del Preprocesamiento CategÃ³rico

Una de las decisiones de diseÃ±o mÃ¡s crÃ­ticas en este proyecto fue cÃ³mo manejar el gran nÃºmero de variables categÃ³ricas (como `gender`, `employment_status`, `region`, etc.).

**El Dilema: Pureza TeÃ³rica vs. Rendimiento PrÃ¡ctico**

1.  **El Enfoque TeÃ³rico (Puro):** TeÃ³ricamente, la mayorÃ­a de las 20 variables agrupadas en la lista `ordinales` (ej. `gender`, `marital_status`, `region`) son **nominales**. El mÃ©todo mÃ¡s puro para un modelo k-NN es aplicarles **One-Hot Encoding (OHE)**. Esto evita crear distancias falsas.

2.  **El Problema PrÃ¡ctico (La MaldiciÃ³n de la Dimensionalidad):** Si aplicÃ¡ramos OHE a estas 20 variables, el nÃºmero de *features* (dimensiones) se dispararÃ­a, pasando de 46 a, potencialmente, mÃ¡s de 100. Para un modelo basado en distancias como k-NN, un aumento tan drÃ¡stico de la dimensionalidad es catastrÃ³fico. El espacio de caracterÃ­sticas se vuelve disperso, la nociÃ³n de "vecino cercano" pierde sentido y el ruido puede dominar la seÃ±al.

**Nuestra DecisiÃ³n MetodolÃ³gica (Basada en Evidencia)**

Decidimos tomar un enfoque prÃ¡ctico, justificado por los resultados de la **permutaciÃ³n por importancia** de las *features*:

* **Variables de Alta Importancia:** Los resultados demuestran que `primary_choice` (importancia: **0.8069**) es, por un margen abrumador, la variable mÃ¡s predictiva. Era **crÃ­tico** codificarla perfectamente. Por lo tanto, esta variable (junto con `secondary_choice`) **sÃ­** fue tratada con OHE.

* **Variables de Baja Importancia:** El resto de las variables categÃ³ricas en disputa (como `gender`: 0.0028, `marital_status`: 0.0037, `region`: -0.0027) mostraron una importancia **casi nula** o incluso negativa (ruido).

Basado en esta evidencia, se tomÃ³ la decisiÃ³n metodolÃ³gica de **NO** aplicar OHE a las variables categÃ³ricas de baja importancia. En su lugar, se agruparon en la lista de `ordinales` y se trataron como un solo valor numÃ©rico, escalÃ¡ndolas con `MinMaxScaler`.

**Asumimos conscientemente el error teÃ³rico** que esto introduce la creaciÃ³n de "distancias falsas". Este error se justifica porque es un costo mucho menor comparado con el efecto destructivo de la maldiciÃ³n de la dimensionalidad que resultarÃ­a de aÃ±adir docenas de columnas de ruido al modelo.
### 4.3 One Hot Encoding de Variables Nominales

**Variables nominales:** `primary_choice`, `secondary_choice`

-   Variables nominales **no tienen orden inherente**
-   Sin OneHot, el modelo asumirÃ­a que `CAND_Demetra (3) estÃ¡ mÃ¡s cerca de CAND_Electra (4)` que de `CAND_Azon (0)`, lo cual es **falso**
-   OneHot crea features binarias independientes, eliminando jerarquÃ­a artificial

### 4.4 Pipeline de Preprocesamiento

El archivo `final_preprocessor.joblib` contiene un `ColumnTransformer` de scikit-learn:

```python
continuas = [
    "age", "household_size", "refused_count", "tv_news_hours", 
    "social_media_hours", "job_tenure_years"
]
ordinales = [
    "gender", "education", "employment_status", "employment_sector", 
    "income_bracket", "marital_status", "has_children", "urbanicity", 
    "region", "voted_last", "party_id_strength", "union_member", 
    "public_sector", "home_owner", "small_biz_owner", "owns_car", 
    "wa_groups", "attention_check", "will_turnout", 
    "preference_strength", "survey_confidence", "trust_media", 
    "civic_participation"
]
nominales_texto = ["primary_choice", "secondary_choice"]

pipe_cont = Pipeline([
    ("imp", SimpleImputer(strategy="median")),
    ("sc",  MinMaxScaler())
])

# Pipeline para Ordinales: Imputar con Mediana, Escalar [0,1]
pipe_ord  = Pipeline([
    ("imp", SimpleImputer(strategy="median")),
    ("sc",  MinMaxScaler())
])

# Pipeline para Nominales: Imputar con Moda, luego One-Hot Encoding
pipe_nom  = Pipeline([
    ("imp", SimpleImputer(strategy="most_frequent")),
    ("ohe", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
])

# ColumnTransformer une todos los pipelines
preprocessor = ColumnTransformer([
    ("cont", pipe_cont, continuas),
    ("ord",  pipe_ord,  ordinales),
    ("nom",  pipe_nom,  nominales_texto),
], remainder="drop")
```

#### Formula matemÃ¡tica MinMaxScaler

Para una variable $x$ con valores mÃ­nimos $x_{min}$ y mÃ¡ximos $x_{max}$:
$$x' = \frac{x - x_{min}}{x_{max} - x_{min}}$$

### 4.5 Variable Objetivo: `intended_vote`

**Pregunta en encuesta:** "Â¿Por cuÃ¡l candidato tiene intenciÃ³n de votar?"

**CodificaciÃ³n:**

```python
y_labels = df["intended_vote"].astype("category")
target_map = dict(enumerate(y_labels.cat.categories))
y = y_labels.cat.codes.values

# Resultado:
# target_map = {
#     0: "CAND_Azon",
#     1: "CAND_Boreal",
#     2: "CAND_Civico",
#     3: "CAND_Demetra",
#     4: "CAND_Electra",
#     5: "CAND_Frontera",
#     6: "CAND_Gaia",
#     7: "CAND_Halley",
#     8: "CAND_Icaro",
#     9: "CAND_Jade"
# }
#
# y = [6, 0, 3, 6, 4, 1, ...]  # CÃ³digos numÃ©ricos
```

**Importante:** Los cÃ³digos (0-9) son **etiquetas categÃ³ricas**, no valores ordinales. k-NN trata cada cÃ³digo como una clase independiente sin asumir jerarquÃ­a (0 no es "menor" que 9).

---

## 5. Decisiones de DiseÃ±o

### 5.1 Â¿Por QuÃ© Distancia Euclidiana?

#### Problema Fundamental sin Escalado

#### FÃ³rmula MatemÃ¡tica

Para dos vectores $\mathbf{a}, \mathbf{b} \in \mathbb{R}^{46}$:

$$d_{euclidiana}(\mathbf{a}, \mathbf{b}) = \sqrt{\sum_{i=1}^{46} (a_i - b_i)^2}$$

#### JustificaciÃ³n TÃ©cnica

| Criterio                          | EvaluaciÃ³n | RazÃ³n                                                               |
| --------------------------------- | ---------- | ------------------------------------------------------------------- |
| **MÃ©trica verdadera**             | âœ…         | Cumple: no-negatividad, simetrÃ­a, desigualdad triangular, identidad |
| **Balance de features**           | âœ…         | Con Ïƒ=1, todas las variables pesan igual                            |
| **Eficiencia computacional**      | âœ…         | O(d) con operaciones vectorizadas NumPy                             |
| **Sensibilidad a outliers**       | âš ï¸         | Mitigada por MinMaxScaler                 |
---

### 5.2 Â¿Por QuÃ© k=19?

#### ExperimentaciÃ³n SistemÃ¡tica

Evaluamos k desde 1 hasta 19 con validaciÃ³n en conjunto de prueba:

| K      | Accuracy   | F1-Score   | InterpretaciÃ³n               |
| ------ | ---------- | ---------- | ---------------------------- |
| 1      | 83.50%     | 0.7844     | Alto varianza - overfitting  |
| 2      | 80.17%     | 0.7305     | Empates frecuentes           |
| 3      | 85.50%     | 0.7966     | Mejor que k=1, aÃºn inestable |
| 5      | 89.50%     | 0.8616     | Mejora significativa         |
| 7      | 90.83%     | 0.8810     | Balance razonable            |
| 9      | 90.50%     | 0.8784     | Ligera caÃ­da                 |
| 11     | 90.83%     | 0.8848     | Estable                      |
| 13     | 91.17%     | 0.8872     | Mejora gradual               |
| 15     | 91.67%     | 0.8935     | Buen balance                 |
| 16     | 92.00%     | 0.8981     | Mejora continua              |
| 17     | 91.83%     | 0.8963     | Ligera caÃ­da                 |
| 18     | 91.33%     | 0.8871     | DegradaciÃ³n                  |
| **19** | **92.83%** | **0.9120** | **âœ… Ã“PTIMO**                |

**Observaciones:**

-   **k=1-5**: Mejora rÃ¡pida (reducciÃ³n de overfitting)
-   **k=7-15**: Mejora gradual (regiÃ³n de balance)
-   **k=16-19**: Pico de rendimiento
-   **k=19**: **MÃ¡ximo global** antes de underfitting

**Con k=19:**

-   âœ… Suficientemente grande para promediar ruido
-   âœ… Suficientemente pequeÃ±o para mantener localidad
-   âœ… Impar (evita empates en clasificaciÃ³n binaria)
-   âœ… Validado empÃ­ricamente con F1-Score=0.9120

### 5.3 Â¿Por QuÃ© Escalar los Datos?

#### Necesidad CrÃ­tica del Escalado

**Problema fundamental:** Variables con diferentes unidades/rangos dominan el cÃ¡lculo de distancia.

#### SoluciÃ³n: MinMaxScaler

**TransformaciÃ³n aplicada:**
$z_i = \frac{x_i - \min_i}{\max_i - \min_i}$

Donde:

-   $x_i$ = valor original de la variable i
-   $\min_i$ = valor mÃ­nimo de la variable i en el dataset de entrenamiento
-   $\max_i$ = valor mÃ¡ximo de la variable i en el dataset de entrenamiento

**Resultado:** Todas las variables estÃ¡n en el rango [0, 1]

### 5.4 Â¿Por QuÃ© ImplementaciÃ³n Manual?

#### Objetivos AcadÃ©micos

1. **ComprensiÃ³n profunda**: Entender cada paso del algoritmo, no solo usarlo como "caja negra"
2. **DemostraciÃ³n de conocimiento**: Probar capacidad de implementar desde cero
3. **Transparencia**: Poder inspeccionar y explicar cada decisiÃ³n del modelo
4. **Control total**: Modificar cualquier aspecto (mÃ©trica, votaciÃ³n, pesos)

### 5.5 Importancia de CaracterÃ­sticas (Permutation Importance)

Para validar la arquitectura del modelo y entender quÃ© variables *realmente* impulsan la predicciÃ³n, se realizÃ³ un anÃ¡lisis de **Permutation Importance**. Esta tÃ©cnica mide cuÃ¡nto cae el F1-Score del modelo si "barajamos" (permutamos) aleatoriamente el valor de una sola variable, rompiendo su relaciÃ³n con el *target*.

**Una alta caÃ­da en el score significa que la variable es muy importante.**

Los resultados demuestran de manera contundente la estrategia de preprocesamiento:

| Variable | Importancia (CaÃ­da en F1-Macro) |
| :--- | :--- |
| **primary\_choice** | **0.806953** |
| voted\_last | 0.010764 |
| social\_media\_hours | 0.009649 |
| household\_size | 0.006344 |
| refused\_count | 0.006294 |
| survey\_confidence | 0.006047 |
| employment\_status | 0.005821 |
| job\_tenure\_years | 0.005048 |
| tv\_news\_hours | 0.004975 |
| attention\_check | 0.004907 |
| preference\_strength | 0.004628 |
| home\_owner | 0.004496 |
| small\_biz\_owner | 0.004365 |
| civic\_participation | 0.004119 |
| marital\_status | 0.003781 |
| party\_id\_strength | 0.003752 |
| age | 0.003389 |
| gender | 0.002840 |
| wa\_groups | 0.002471 |
| trust\_media | 0.001957 |
| has\_children | 0.001862 |
| public\_sector | 0.001735 |
| education | 0.001670 |
| union\_member | 0.001658 |
| income\_bracket | 0.000750 |
| secondary\_choice | 0.000308 |
| owns\_car | 0.000182 |
| employment\_sector | 0.000073 |
| undecided | 0.000000 |
| urbanicity | -0.001058 |
| will\_turnout | -0.002598 |
| region | -0.002770 |