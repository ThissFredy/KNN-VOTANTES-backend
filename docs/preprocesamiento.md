# üöÄ Informe de Metodolog√≠a: Preprocesamiento y Creaci√≥n del Modelo k-NN

## 1. Introducci√≥n: El Problema de los Dos Modelos

Este script es el n√∫cleo de la preparaci√≥n de datos del proyecto. Su objetivo es tomar el dataset crudo y ruidoso (`voter_intentions_3000.csv`) y convertirlo en un archivo limpio, optimizado y listo para producci√≥n (`voter_intentions_COMPLETED_PROCESSED.csv`).

Para lograr esto, el script resuelve **dos problemas de Machine Learning distintos** en secuencia:

1.  **Problema de Propagaci√≥n:** El dataset original tiene un 75% de etiquetas faltantes ("Undecided"). Debemos usar el 25% de datos "conocidos" (755 filas) para "adivinar" (propagar) las etiquetas del 75% restante.
2.  **Problema de Predicci√≥n:** Una vez que tenemos un dataset completo (3000 filas), debemos optimizarlo (seleccionando solo las features √∫tiles) y encontrar el mejor hiperpar√°metro (`k`) para el modelo final que se usar√° en la API.

---

## 2. Fase 1: Carga y Definici√≥n de Pipelines (Pasos 1-4)

### Qu√© se hace

Se carga el dataset crudo (`voter_intentions_3000.csv`) y se definen todas las columnas por tipo: `continuas` (ej. `age`), `ordinales` (ej. `education`) y `nominales_texto` (ej. `primary_choice`).

Luego, se crea un `preprocessor` de `sklearn` que define una "receta" de limpieza para cada tipo de dato.

### El Porqu√© (La Raz√≥n)

El dataset crudo no puede ser usado directamente por un modelo k-NN. El modelo fallar√≠a porque:

1.  **Contiene Texto:** k-NN necesita n√∫meros para calcular distancias. No puede restar `"CAND_Azon"` de `"CAND_Gaia"`.
2.  **Contiene `NaN` (Datos Faltantes):** La matem√°tica de la distancia (ej. `50 - NaN`) falla con valores nulos.
3.  **Tiene Escalas Diferentes:** Una feature como `age` (rango 18-84) dominar√≠a injustamente a una feature como `public_sector` (rango 0-1) en el c√°lculo de la distancia.

### Conceptos Matem√°ticos Clave

-   **Imputaci√≥n (Mediana/Moda):** Se usa la **Mediana** (valor central) para las columnas num√©ricas porque es robusta a _outliers_ (valores at√≠picos). Se usa la **Moda** (valor m√°s frecuente) para las columnas de texto.
-   **Escalado (MinMaxScaler):** Normaliza todas las features num√©ricas al mismo rango [0, 1] para que contribuyan de forma justa a la distancia.
    $X_{\text{scaled}} = \frac{X - X_{\text{min}}}{X_{\text{max}} - X_{\text{min}}}$
-   **One-Hot Encoding (OHE):** Convierte una columna nominal (ej. `primary_choice`) en `N` columnas binarias (0 o 1). Esto es crucial porque evita crear un orden num√©rico falso (ej. `Azon=1`, `Gaia=2` implicar√≠a falsamente que `Gaia` es "mayor" que `Azon`).

---

## 3. Fase 2: Hallar K para Propagaci√≥n y Etiquetado (Paso 5)

### Qu√© se hace

Se busca el valor `k` √≥ptimo para la **tarea de propagaci√≥n**.

1.  Se divide el 25% de datos "conocidos" (`df_known`, 755 filas) en un set de entrenamiento (`X_train_known`) y uno de validaci√≥n (`X_test_known`).
2.  Se aplica el `preprocessor` (Imputaci√≥n, Escalado, OHE) a ambos sets.
3.  Se itera `k` de 1 a 20, entrenando un `KNeighborsClassifier` en el set de entrenamiento y midiendo su **F1-Score (Macro)** en el set de validaci√≥n.
4.  El `k` con el F1-Score m√°s alto (`k=19`) se usa para entrenar un modelo final con _todos_ los datos conocidos (`X_known`, 755 filas).
5.  Este modelo final predice las etiquetas para las 2245 filas "desconocidas" (`X_unknown`).

### El Porqu√© (La Raz√≥n)

No podemos asumir que un `k` arbitrario (como `k=8`) es el mejor. Esta tarea es dif√≠cil (pocos datos) y cr√≠tica. Encontrar el `k` √≥ptimo nos da la mayor confianza de que las etiquetas que estamos "inventando" para los indecisos son lo m√°s precisas posible.

-   **Resultado de Consola:** `Mejor K (para propagaci√≥n) encontrado: K=19 con F1-macro=0.8931`

### Conceptos Matem√°ticos Clave

-   **M√©trica (F1-Score Macro):** Se elige esta m√©trica sobre la "Accuracy" porque nuestro dataset de 755 filas es **desbalanceado** (algunos candidatos tienen m√°s votantes que otros).
    -   $Precisi√≥n = \frac{\text{Verdaderos Positivos}}{\text{Todos los Positivos Predichos}}$
    -   $Recall = \frac{\text{Verdaderos Positivos}}{\text{Todos los Positivos Reales}}$
    -   $F1 = 2 \times \frac{\text{Precisi√≥n} \times \text{Recall}}{\text{Precisi√≥n} + \text{Recall}}$
    -   El **"Macro"** calcula el F1-Score para cada candidato por separado y luego toma el promedio simple. Esto asegura que el rendimiento en candidatos minoritarios es tan importante como en los mayoritarios.

---

## 4. Fase 3: Selecci√≥n de Features Relevantes (Pasos 6-7)

### Qu√© se hace

Se unen los datos "conocidos" (755) y los "predichos" (2245) para crear `df_completed` (3000 filas). Luego, se ejecuta una **Permutaci√≥n por Importancia** sobre este dataset completo para descubrir qu√© features son _realmente_ √∫tiles.

### El Porqu√© (La Raz√≥n)

El `df_completed` es una simulaci√≥n de nuestro dataset de producci√≥n. Ahora el problema ha cambiado: ya no es predecir "Indeciso", sino predecir _entre 10 candidatos_. Necesitamos saber qu√© features contienen "se√±al" (informaci√≥n √∫til) y cu√°les son "ruido" (informaci√≥n in√∫til o perjudicial).

### Conceptos Matem√°ticos Clave

-   **Permutation Importance:** Un m√©todo robusto para medir la utilidad de una feature.
    1.  El modelo calcula el F1-Score base (ej. 0.90) en el set de prueba.
    2.  Luego, "baraja" (permuta) aleatoriamente solo una columna (ej. `age`) y vuelve a calcular el F1-Score (ej. 0.88).
    3.  $Importancia_{\text{age}} = 0.90 - 0.88 = 0.02$ (Es √∫til).
    4.  Si el score _empeora_ (ej. 0.87), la importancia es positiva.
    5.  Si el score _mejora_ (ej. 0.91), la feature es _perjudicial_ (importancia negativa).
-   **Resultado de Consola:** El an√°lisis (`=== Importancia de Features ===`) mostr√≥ que **28 de las 31** features ten√≠an un impacto positivo. `primary_choice` fue la m√°s importante (0.806), mientras que 3 features (`urbanicity`, `will_turnout`, `region`) resultaron ser perjudiciales (negativas) y, por lo tanto, se descartan.

---

## 5. Fase 4: Creaci√≥n del Dataset Final (Paso 8)

### Qu√© se hace

Se toma la lista de 28 features positivas. Se crea un `final_preprocessor` que solo procesa esas 28 features. Este preprocesador se usa para transformar las 3000 filas del `df_completed` y el resultado se guarda como `voter_intentions_COMPLETED.csv`.

### El Porqu√© (La Raz√≥n)

Este es el **artefacto de producci√≥n**. Es el archivo final, 100% limpio (sin NaNs), procesado (OHE y escalado) y optimizado (solo features √∫tiles) que nuestra API cargar√° en memoria. Esto hace que la API sea extremadamente r√°pida, ya que no tiene que hacer ning√∫n preprocesamiento en vivo.

-   **Resultado de Consola:** `El shape del archivo es: (3000, 47)`.
    -   Esto significa que nuestras 28 features "crudas" seleccionadas se convirtieron en 46 columnas "procesadas" (despu√©s de OHE) + 1 columna de target.

---

## 6. Fase 5: Hallar K √ìptimo para Producci√≥n (Paso 9)

### Qu√© se hace

Ahora que tenemos el dataset de producci√≥n (`...COMPLETED.csv`), cargamos _ese_ archivo y ejecutamos un segundo "M√©todo del Codo" (probar k de 1 a 20) sobre √©l.

### El Porqu√© (La Raz√≥n)

El "terreno" ha cambiado. El primer `k` (k=19) se encontr√≥ en un dataset _diferente_ (755 filas, 30+ features). Debemos encontrar el `k` que sea √≥ptimo para los **datos exactos que usar√° la API** (3000 filas, 46 features). Este es el hiperpar√°metro final y validado para nuestro modelo.

### Conceptos Matem√°ticos Clave

-   **M√©todo del Codo / Meseta:** Al graficar el F1-Score contra `k`, buscamos el "codo" (la √∫ltima subida significativa) o el inicio de la "meseta" (donde el score se estabiliza).
-   **Resultado de Consola:** `Mejor K encontrado: K=19 con F1-score=0.9120`.
    -   Este resultado confirma que `k=19` es el valor m√°s robusto y preciso para el modelo final, logrando un F1-Score (macro) extremadamente alto de **91.2%**. Este ser√° el `k` que usaremos en producci√≥n.
